%%%%%%%%%%%%%%%%%%%%
% This file is automatically generated. Refer to the markdown source.
%%%%%%%%%%%%%%%%%%%%

\documentclass[manuscript,authordraft]{acmart}
\usepackage[utf8]{inputenc}
% ACM options first in the order they appear in acmguide.pdf


  \acmConference[]{Soft. Eng. 4 Res. Sci.}{July 23--27, 2023}{Portland,
OR}

\title{Wanted: standards for automatic reproducibility of computational
experiments}

\author{Samuel Grayson}
  
  \email{grayson5@illinois.edu}
  \orcid{0000-0001-5411-356X}
      \affiliation{%
    %
    \institution{University of Illinois Urbana-Champaign}%
    \department{Department of Computer Science}%
    \streetaddress{201 North Goodwin Avenue MC 258}%
    \city{Urbana}%
    \state{IL}%
    \postcode{61801-2302}%
    \country{USA}%
    }
    
  
\author{Joshua Teves}
  
  \email{jbteves@sandia.gov}
  \orcid{0000-0002-7767-0067}
      \affiliation{%
    %
    \institution{Sandia National Laboratories}%
    \department{Software Engineering and Research Department}%
    \streetaddress{1515 Eubank Blvd SE1515 Eubank Blvd SE}%
    \city{Albuquerque}%
    \state{NM}%
    \postcode{87123}%
    \country{USA}%
    }
    
  
\author{Reed Milewicz}
  
  \email{rmilewi@sandia.gov}
  \orcid{0000-0002-1701-0008}
      \affiliation{%
    %
    \institution{Sandia National Laboratories}%
    \department{Software Engineering and Research Department}%
    \streetaddress{1515 Eubank Blvd SE1515 Eubank Blvd SE}%
    \city{Albuquerque}%
    \state{NM}%
    \postcode{87123}%
    \country{USA}%
    }
    
  
\author{Daniel S. Katz}
  
  \email{dskatz@illinois.edu}
  \orcid{0000-0001-5934-7525}
      \affiliation{%
    %
    \institution{University of Illinois Urbana-Champaign Department of
Computer Science}%
    \department{Department of Computer Science}%
    \streetaddress{201 North Goodwin Avenue MC 258}%
    \city{Urbana}%
    \state{IL}%
    \postcode{61801-2302}%
    \country{USA}%
    }
    
  
\author{Darko Marinov}
  
  \email{marinov@illinois.edu}
  \orcid{0000-0001-5023-3492}
      \affiliation{%
    %
    \institution{University of Illinois Urbana-Champaign}%
    \department{Department of Computer Science}%
    \streetaddress{201 North Goodwin Avenue MC 258}%
    \city{Urbana}%
    \state{IL}%
    \postcode{61801-2302}%
    \country{USA}%
    }
    
  



\acmYear{2023}









\setcopyright{none}


\settopmatter{printacmref=false}



% \RequirePackage[
%   datamodel=acmdatamodel,
%   style=acmnumeric, % use style=acmauthoryear for publications that require it
% ]{biblatex}
% % \addbibresource{main.bib.bib}
% 
\usepackage{hyperref}



\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}


\usepackage{microtype}





\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi



\date{2023-05-22}


\begin{document}



\maketitle

\renewcommand{\shortauthors}{Grayson et al.}


\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

A computational experiment is reproducible if another team using the
same experimental infrastructure can make a measurement that concurs
with the original. Investing in reproducibility can improve
trustworthiness of scientific results, unlock productivity, and enable
reusability and community support\cite{ivie2018reproducibility}. In
practice, reproducers often need to manually work with the code to see
how to build necessary libraries, configure parameters, find data, and
invoke the experiment; it is not \emph{automatic}.

In this work, we consider the use of specification languages that would
provide machine-readable metadata on how to interact with a piece of
software. It is not enough for the language to merely contain a run
command in a heap of other commands; e.g., a Makefile which defines a
rule for executing the experiment alongside rules for compiling
intermediate pieces is not sufficient, because there is no
machine-readable way to know which of the Make rules executes the
experiment. Being able to automatically identifying the ``main'' command
which executes the experiment, for instance, would be very useful for
those seeking to reproduce results from past experiments or reusing
experiments to address new use cases. Moreover, from a research
perspective, having a standardized way to run many different codes at
scale would open new avenues for data mining research on reproducibility
(c.f., \cite{collberg_repeatability_2016}).

\hypertarget{towards-specification-standards-for-automated-reproducibility-assessment}{%
\section{Towards Specification Standards for Automated Reproducibility
Assessment}\label{towards-specification-standards-for-automated-reproducibility-assessment}}

At present, there are a diverse range of solutions for expressing how a
code should be run, including bash scripts, environment management
specifications (e.g., Spack, Nix, Python Virtualenv), continuous
integration scripts, workflows, and container specifications. In our own
research on reproducibility of scientific codes, as we scale up our
studies to include many different codes, keeping track of how to execute
each one becomes very complicated. Moreover, when a code fails to run or
deliver reproducible results, it is difficult to assess whether there is
a fault with the code or whether we failed to run the code in the
correct way. While we do not expect (or recommend) that the scientific
software community converge on a single solution for executing codes, we
do see value in having a standard way of documenting how each code ought
to be run. Such a standard could be implemented as a linked-data
ontology like the semantic web. This would enable us to build on a rich
set of ontologies for describing digital and physical resources
(RO-crate \cite{soiland-reyes_packaging_2022}, Dublin Core metadata
terms \cite{weibel_dublin_2000}, Description of a Project
\cite{wilder-james_description_2017}, nanopublications
\cite{groth_anatomy_2010}, Citation Typing Ontology
\cite{shotton_cito_2010}, Documnet Componnets Ontology
\cite{constantin_document_2016}) and leverage the ecosytem of existing
tools for ontology management (Protégé editor, SHACL, SHEX).

At a very basic level, one could specify available commands and the
purpose that they serve (e.g., run make to compile underlying libraries
and run main.py to generate figures). But more than just that, a linked
data specification standard would enable researchers to link inputs and
outputs of codes directly to claims made in publications. With such a
specification, any person (or program) should be able to execute the
experiments which generate figures or claims in an accompanying paper.
For example, the CiTO vocabulary \cite{shotton_cito_2010} can be used to
how the result of a process is used as evidence in a specific
publication. These references could be connected to other references of
the same publication on the semantic web. The description can be even
more granular, such as by using the DoCO vocabulary
\cite{constantin_document_2016} to point to specific elements (e.g.,
figures) within a document, or using Nanopublication
\cite{groth_anatomy_2010} to reference specific scientific claims.
RO-crate \cite{soiland-reyes_wf4ever_2013} has terms for describing
dependencies between steps, which can be used to encode dependent steps
or specify the computational environment. This would not replace any of
the underlying tools, libraries, or frameworks used at present, but it
would provide a common vocabulary for explaining how those tools,
libraries, and frameworks are employed. Such a specification language
could also be used to set bounds on the parameters of the experiment,
such as the range of valid values or a list of toggleable parameters;
this would enable downstream automated experiments like parameter-space
search studies, multi-fidelity uncertainty quantification, and
outcome-preserving input minimization.

\hypertarget{incentivizing-adoption-of-specifications}{%
\section{Incentivizing Adoption of
Specifications}\label{incentivizing-adoption-of-specifications}}

With a defined specification language in place, tooling could be built
to make it as easy as possible to generate a specification document. For
example, an execution description could be ``captured'' from an
interactive shell session with the user. They would invoke a shell that
records every command, its exit status, its read-files, and its
write-files (using syscall interposition); When the user exits, the
shell will create a directed acyclic graph based on the read-files and
write-files, and for each output file that is not consumed by another
command, the shell would prompt the user to describe that command's
purpose. Alternatively, in cases where the description cannot be
captured by an interactive shell session, it may be possible to encode a
system of logic that humans would use to deduce the experimental
structure; this is similar to the approach taken by FlaPy
\cite{gruber_empirical_2021}, a large-scale re-execution study for
Python unittests, to install the Python environment.

Meanwhile, conferences and publishers could promote the use of such
standard specifications as part of reproducibility requirements for
publishing. To get an artifact evaluation badge, normally computational
scientists would have to write a natural language description of what
the software environment, what the commands are, how to run them, and
where does the data end up; meanwhile, an artifact evaluator has to
read, interpret, and execute their description by hand. An execution
description could make this nearly automatic; if a execution description
exists, the artifact evaluator uses an executor which understands the
language and runs all of the commands that reference the manuscript in
their \texttt{purpose} tag. The only manual labor is comparing these
results to those in the paper. Even that comparison can be simplified,
if the last step in the execution description outputs a boolean
representing ``is the hypothesis proven?''; the reviewer just needs to
see that all of these output ``true''.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

In this position paper, we argue that developing common standards for
specifying how computational experiments should be run to get
reproducible results would benefit the scientific community. It presents
a compromise where different teams can implement their codes in whatever
way they see fit while enabling others to easily run them. This would
lead to greater productivity in the (re)use of scientific experiments,
empower developers to build tools that leverage those common
specifications, and enable software engineering researchers to study
reproducibility at scale.

\hypertarget{appendix-i-example-document}{%
\section{Appendix I: Example
document}\label{appendix-i-example-document}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{\textless{}?xml}\OtherTok{ version=}\StringTok{"1.0"}\FunctionTok{?\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{rdf:RDF}
\OtherTok{  xmlns:rdf=}\StringTok{"http://www.w3.org/1999/02/22{-}rdf{-}syntax{-}ns\#"}
\OtherTok{  xmlns=}\StringTok{"https://example.com/"}
\NormalTok{\textgreater{}}
\CommentTok{\textless{}!{-}{-}}
\CommentTok{The RDF tag defines several XML namespaces.}

\CommentTok{In this document "rdf:label" refers to http://www.w3.org/1999/02/22{-}rdf{-}syntax{-}ns\#label}
\CommentTok{{-}{-}\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{process}\OtherTok{ rdf:ID=}\StringTok{"make"}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{command}\NormalTok{\textgreater{}make libs\textless{}/}\KeywordTok{command}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{purpose}\NormalTok{\textgreater{}compiles libraries\textless{}/}\KeywordTok{purpose}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{process}\NormalTok{\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{process}\OtherTok{ rdf:ID=}\StringTok{"figures"}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{command}\NormalTok{\textgreater{}python3 main.py\textless{}/}\KeywordTok{command}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{purpose}\NormalTok{\textgreater{}generates figures\textless{}/}\KeywordTok{purpose}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{depends\_on}\OtherTok{ rdf:resource=}\StringTok{"\#make"}\NormalTok{ /\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{process}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{rdf:RDF}\NormalTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{how-to-get-automatic-reproducibility}{%
\section{How to get automatic
reproducibility}\label{how-to-get-automatic-reproducibility}}

This is not the final proposal for the complete vocabulary; the
peer-review process is not well-suited to iterate on technical details.
The point of this article is to argue that the community should spend
effort developing this vocabulary.

\hypertarget{language-description}{%
\subsection{Language description}\label{language-description}}

At a very basic level, one could have commands and the purpose that they
serve:

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}}\KeywordTok{process}\OtherTok{ rdf:ID=}\StringTok{"make"}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{command}\NormalTok{\textgreater{}make libs\textless{}/}\KeywordTok{command}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{purpose}\NormalTok{\textgreater{}compiles libraries\textless{}/}\KeywordTok{purpose}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{process}\NormalTok{\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{process}\OtherTok{ rdf:ID=}\StringTok{"figures"}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{command}\NormalTok{\textgreater{}python3 main.py\textless{}/}\KeywordTok{command}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{purpose}\NormalTok{\textgreater{}generates figures\textless{}/}\KeywordTok{purpose}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{depends\_on}\OtherTok{ target=}\StringTok{"\#make"}\NormalTok{ /\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{process}\NormalTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

\normalsize

The process labeled \texttt{figures} references the process labeled make
as a prerequisite using the XML RDF reference syntax
\cite{gandon_rdf_2014}. The example depects a simple
\texttt{depends\_on} predicate, but one might use the albeit more
complex wfdesc vocabulary \cite{soiland-reyes_wf4ever_2013} to describe
these dependencies. One can view specifying the software environment as
just prerequisite steps in the computational DAG{[}\^{}define-dag{]}.
The purpose of an execution language is not to usurp the build-system or
workflow engine, which both already handle task DAGs; there must be some
minimal support for DAGs just for the cases where the DAG of tasks is
not already encoded in a build-system or workflow engine.

{[}define-dag{]}: A computaitonal directed acyclic graph (DAG) is a set
of programs and pairs of programs (called links), where each link
indicates the output of one program is the input to the other program.
This is the basic concept of GNU Make, workflows, and build systems.

While we could define conventions around what to name the content of the
``purpose'' tag, it would be more powerful if the language could link
directly to the claims in the publication. Purpose blocks might look
more like this:

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}}\KeywordTok{purpose}\OtherTok{ rdf:ID=}\StringTok{"pub"}\NormalTok{\textgreater{}}
  \CommentTok{\textless{}!{-}{-} Links to an entire publication {-}{-}\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{cito:isCitedAsEvidenceBy}\OtherTok{ rdf:resource=}\StringTok{"https://doi.org/10.1234/123456789"}\NormalTok{ /\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{purpose}\NormalTok{\textgreater{}}
\NormalTok{...}
\NormalTok{\textless{}}\KeywordTok{purpose}\OtherTok{ rdf:ID=}\StringTok{"fig{-}pub"}\NormalTok{\textgreater{}}
  \CommentTok{\textless{}!{-}{-} Links to a specific figure within a publication {-}{-}\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{prov:generated}\NormalTok{\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{doco:figure}\NormalTok{\textgreater{}}
\NormalTok{      \textless{}}\KeywordTok{dc:title}\NormalTok{\textgreater{}Figure 2b\textless{}/}\KeywordTok{dc:title}\NormalTok{\textgreater{}}
\NormalTok{      \textless{}}\KeywordTok{dc:isPartOf}\OtherTok{ rdf:resource=}\StringTok{"https://doi.org/10.1234/123456789"}\NormalTok{ /\textgreater{}}
\NormalTok{    \textless{}/}\KeywordTok{doco:figure}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}/}\KeywordTok{prov:generated}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{purpose}\NormalTok{\textgreater{}}
\NormalTok{...}
\NormalTok{\textless{}}\KeywordTok{purpose}\OtherTok{ rdf:ID=}\StringTok{"claim"}\NormalTok{\textgreater{}}
  \CommentTok{\textless{}!{-}{-} Describes a specific assertion {-}{-}\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{cito:supports}\NormalTok{\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{wikibase:Statement}\NormalTok{\textgreater{}}
\NormalTok{      \textless{}}\KeywordTok{subject}\OtherTok{ rdf:resource=}\StringTok{"https://www.wikidata.org/entity/Q12156"}\NormalTok{ /\textgreater{} }\CommentTok{\textless{}!{-}{-} Q12156 refers to malaria {-}{-}\textgreater{}}
\NormalTok{      \textless{}}\KeywordTok{predicate}\OtherTok{ rdf:resource=}\StringTok{"http://www.wikidata.org/prop/P1060"}\NormalTok{ /\textgreater{} }\CommentTok{\textless{}!{-}{-} P1060 refers to disease transmission process (read: "is transmitted by") {-}{-}\textgreater{}}
\NormalTok{      \textless{}}\KeywordTok{object}\OtherTok{ rdf:resource=}\StringTok{"https://www.wikidata.org/entity/Q15304532"}\NormalTok{ /\textgreater{} }\CommentTok{\textless{}!{-}{-} Q15304532 refers to mosquitoes {-}{-}\textgreater{}}
\NormalTok{    \textless{}/}\KeywordTok{wikibase:Statement}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}/}\KeywordTok{cito:supports}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{purpose}\NormalTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

\normalsize

The block labeled \texttt{pub} uses the CiTO vocabulary
\cite{shotton_cito_2010} to explain that the result of that process is
used as evidence in a specific publication. If the publisher hosts an
RDF description at the URL ``https://doi.org/10.1234/123456789'' when
the HTTP request content-type header is \texttt{application/rdf+xml},
then this creates a web of linked data. The publisher may have the
title, authors, date published, and other metadata using Dublin Core
metadata terms \cite{weibel_dublin_2000}, for example. This is the dream
of linked data: machine-readable data by different authors hosted in
different locations linking together seamlessly. Even if the publisher
does not have an RDF+XML description, third parties can make claims
about ``https://doi.org/10.1234/123456789'', although those claims would
not be as easily discoverable. The purpose description can be even more
granular, using the DoCO vocabulary \cite{constantin_document_2016},
which describes documents, as shown in the block labeled
\texttt{fig-pub}.

One could even reference a Nanopublication, which is a semantic web
description of the scientific claim \cite{groth_anatomy_2010}, as in the
block labeled \texttt{claim}. The claim is itself a
subject-predicate-object triple, in this case relating identifiers from
Wikidata \cite{erxleben_introducing_2014} to express ``malaria is
transmitted by mosquitoes''.

With this complete, anyone should be able to execute the experiments
which supported publications, figures, or claims in the paper if they
are labeled by the computational scientist in this language.

In addition to specifying the computational environment and command to
run, this language is an ideal candidate to also describe the parameters
of the experiment, perhaps using wfdesc
\cite{soiland-reyes_wf4ever_2013}. One could specify range of valid
values or list options. With this complete, one can even do automated
parameter-space search studies, multi-fidelity uncertainty
quantification, automated outcome-preserving input minimization, and
other automatic experiments.

\hypertarget{making-easy-on-ramps-for-adoption}{%
\section{Making easy on-ramps for
adoption}\label{making-easy-on-ramps-for-adoption}}

The execution language should seek to describe existing software
frameworks, not replace them. In particular, execution should not
replace workflow engines. They should instead be wrapped as
process-nodes within the execution language. Computational experiments
can continue using their existing build-system and workflow.

A execution description could even be ``captured'' from an interactive
shell session with the user. They would invoke a shell that records
every command, its exit status, its read-files, and its write-files
(using syscall interposition). The user would execute their build-system
like normal within this shell. When the user exits, the shell will
create a DAG based on the read-files and write-files. For each output
file that is not consumed by another command, the shell would prompt the
user to describe that command's purpose. Finally, the shell would output
a execution description.

In cases where the description cannot be captured by an interactive
shell session, one can encode the system of logic that humans would use
to deduce the experimental structure. This is similar to the approach
taken by FlaPy \cite{gruber_empirical_2021}, a large-scale re-execution
study for Python unittests, to install the Python environment. For
example,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If \texttt{shell.nix}, \texttt{flake.nix}, or \texttt{environment.yml}
  exists, then use Nix, Nix flakes, or Conda as the environment for
  future commands.
\item
  If the environment is not already set, and \texttt{requirements.txt}
  exists, use Pip and Virtualenv as the environment for future commands.
\item
  If \texttt{CMakeLists.txt} exists, set \texttt{cmake} and
  \texttt{make} as commands.
\item
  If \texttt{configure.sh} exists, set \texttt{./configure} and
  \texttt{make} as command.
\item
  If \texttt{Makefile} exists, set \texttt{make} as command. \ldots{}
\end{enumerate}

In the best case, the execution description would be uploaded to the
same repository or location that contains the source for the
computational experiment. This way it can be maintained and used by the
original developers, and it is easily discoverable by users.
Alternatively, execution descriptions could be placed in a central
execution-description repository owned by software-engineering
researchers. The execution description is linked data, so it can live
anywhere, and existing strategies for finding, filtering, and trusting
linked data sets would work for execution descriptions.

This is similar to the approach taken by Python for type annotations.
Type annotations are easiest to maintain in the original repository, but
if the original repository rejects type annotations, there is still a
home for them in typeshed.

Is this another competing standard? It is something extra users have to
do, but it does not attempt to displace their existing practice.

\hypertarget{incentives-for-computational-scientists-to-maintain-execution-descriptions}{%
\section{Incentives for computational scientists to maintain execution
descriptions}\label{incentives-for-computational-scientists-to-maintain-execution-descriptions}}

Computational scientists are incentivized to describe their project this
way to benefit from the work of software-engineering researchers. When
software-engineering researchers do a large-scale execution study, it is
a ``free'' reproduction of their work.

To get an artifact evaluation badge, normally computational scientists
would have to write a natural language description of what the software
environment, what the commands are, how to run them, and where does the
data end up. The artifact evaluator has to read, interpret, and execute
their description by hand. An execution description could make this
nearly automatic; if a execution description exists, the artifact
evaluator uses an executor which understands the language and runs all
of the commands that reference the manuscript in their \texttt{purpose}
tag. The only manual labor is comparing these results to those in the
paper. Even that comparison can be simplified, if the last step in the
execution description outputs a boolean representing ``is the hypothesis
proven?''; the reviewer just needs to see that all of these output
``true''.



%%\printbibliography
%
\bibliographystyle{plain}
\bibliography{main.bib}

\end{document}
