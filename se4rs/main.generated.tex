%%%%%%%%%%%%%%%%%%%%
% This file is automatically generated. Refer to the markdown source.
%%%%%%%%%%%%%%%%%%%%

\documentclass[manuscript,authordraft]{acmart}
\usepackage[utf8]{inputenc}
% ACM options first in the order they appear in acmguide.pdf


  \acmConference[]{Software Engineering for Research Software}{July
23--27, 2023}{Portland, OR, USA}

\title{Wanted: standards for automatic reproducibility of computational
experiments}

\author{Samuel Grayson}
  
  \email{grayson5@illinois.edu}
  \orcid{0000-0001-5411-356X}
      \affiliation{%
    %
    \institution{University of Illinois Urbana-Champaign}%
    \department{Department of Computer Science}%
    \streetaddress{201 North Goodwin Avenue MC 258}%
    \city{Urbana}%
    \state{IL}%
    \postcode{61801-2302}%
    \country{USA}%
    }
    
  
\author{Joshua Teves}
  
  \email{jbteves@sandia.gov}
  \orcid{0000-0002-7767-0067}
      \affiliation{%
    %
    \institution{Sandia National Laboratories}%
    \department{Software Engineering and Research Department}%
    \streetaddress{1515 Eubank Blvd SE1515 Eubank Blvd SE}%
    \city{Albuquerque}%
    \state{NM}%
    \postcode{87123}%
    \country{USA}%
    }
    
  
\author{Reed Milewicz}
  
  \email{rmilewi@sandia.gov}
  \orcid{0000-0002-1701-0008}
      \affiliation{%
    %
    \institution{Sandia National Laboratories}%
    \department{Software Engineering and Research Department}%
    \streetaddress{1515 Eubank Blvd SE1515 Eubank Blvd SE}%
    \city{Albuquerque}%
    \state{NM}%
    \postcode{87123}%
    \country{USA}%
    }
    
  
\author{Daniel S. Katz}
  
  \email{dskatz@illinois.edu}
  \orcid{0000-0001-5934-7525}
      \affiliation{%
    %
    \institution{University of Illinois Urbana-Champaign Department of
Computer Science}%
    \department{Department of Computer Science}\department{National
Center for Supercomputing Applications}\department{Deparment of
Electrical and Computer Engineering}\department{School of Information
Sciences}%
    \streetaddress{201 North Goodwin Avenue MC 258}%
    \city{Urbana}%
    \state{IL}%
    \postcode{61801-2302}%
    \country{USA}%
    }
    
  
\author{Darko Marinov}
  
  \email{marinov@illinois.edu}
  \orcid{0000-0001-5023-3492}
      \affiliation{%
    %
    \institution{University of Illinois Urbana-Champaign}%
    \department{Department of Computer Science}%
    \streetaddress{201 North Goodwin Avenue MC 258}%
    \city{Urbana}%
    \state{IL}%
    \postcode{61801-2302}%
    \country{USA}%
    }
    
  



\acmYear{2023}









\setcopyright{none}


\settopmatter{printacmref=false}



% \RequirePackage[
%   datamodel=acmdatamodel,
%   style=acmnumeric, % use style=acmauthoryear for publications that require it
% ]{biblatex}
% % \addbibresource{main.bib.bib}
% 
\usepackage{hyperref}



\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}


\usepackage{microtype}





\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi



\date{2023-05-22}


\begin{document}



\maketitle

\renewcommand{\shortauthors}{Grayson et al.}


\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

A computational experiment is reproducible if another team using the
same experimental infrastructure can make a measurement that concurs
with the original. In practice, reproducers will still need to look at
the code by hand to see how to build necessary libraries, configure
parameters, find data, and invoke the experiment; it is not
\emph{automatic}. To enable automatic reproducibility, one would need a
description language which could list the relevant commands with
machine-readable metadata attached to describe what the command does. It
is not enough for the language to merely contain this command in a heap
of other commands; e.g., a Makefile which defines a rule for executing
the experiment alongside rules for compiling intermediate pieces is not
sufficient, because there is no machine-readable way to know which of
the Make rules executes the experiment.

Automatically identifying the ``main'' command which executes the
experiment is critical for:

\begin{itemize}
\item
  \textbf{Artifact evaluators}: With manual reproducibility, artifact
  evaluators spend time learning how to set up, configure, build, and
  review the artifact. Automatic reproducibility would be the canonical
  place for experiment computational scientists to concisely communicate
  these steps. Unlike \texttt{README.txt} it would be human- and
  machine-readable.
\item
  \textbf{Users seeking to re-execute with different parameters}: With
  manual reproducibility, users have to dig through the experiment's
  documentation or, more likely, source code to discover how to supply
  parameters. Automatic reproducibility can also specify how to set
  these parameters.
\item
  \textbf{Large-scale re-execution experiments}: Collberg and Proebsting
  \cite{collberg_repeatability_2016} do a large-scale study of
  repeatability of computational experiments in computer science with
  manual effort. While their results are seminal, it is difficult to
  repeat in other domains or extend that experiment without spending a
  huge amount of human-hours figuring out how to run experiments. If
  Collberg and Proebsting or some other software-engineering researchers
  \emph{do} embark to figure out how to run a certain experiment, there
  is standardized way for them to share their steps with other
  researchers.
\end{itemize}

\hypertarget{how-to-get-automatic-reproducibility}{%
\section{How to get automatic
reproducibility}\label{how-to-get-automatic-reproducibility}}

This is not the final proposal for the complete vocabulary; the
peer-review process is not well-suited to iterate on technical details.
The point of this article is to argue that the community should spend
effort developing this vocabulary.

\hypertarget{semantic-web}{%
\subsection{Semantic web}\label{semantic-web}}

This language could be implemented as a vocabulary for linked data in
the semantic web. Linked data is preferrable for these reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linked data is open to extensions.
\item
  It is possible to link to other resources in linked data.
\item
  There is already a rich set of ontologies for describing digital and
  physical resources (RO-crate, wf4prov, software project description,
  scientific hypotheses, CiTO) in linked data.
\item
  There is already a rich ecosystem for authoring ontologies and
  validating documents within those ontologies.
\end{enumerate}

Linked data is already used for other long-term preservation standards,
such as RO-crate. The template of RDF/XML looks like this:

\small

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{\textless{}?xml}\OtherTok{ version=}\StringTok{"1.0"}\FunctionTok{?\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{rdf:RDF}\OtherTok{ xmlns:rdf=}\StringTok{"http://www.w3.org/1999/02/22{-}rdf{-}syntax{-}ns\#"}
\OtherTok{         xmlns:cito=}\StringTok{"http://purl.org/spar/cito"}
\OtherTok{         xmlns:doco=}\StringTok{"http://purl.org/spar/doco/2015{-}07{-}03"}
\OtherTok{         xmlns:prov=}\StringTok{"https://www.w3.org/TR/2013/PR{-}prov{-}o{-}20130312/"}
\OtherTok{         xmlns:wfdesc=}\StringTok{"http://purl.org/wf4ever/wfdesc\#"}
\OtherTok{         xmlns=}\StringTok{"http://example.org/execution{-}description/1.0"}\NormalTok{ \textgreater{}}
\NormalTok{...}
\NormalTok{\textless{}/}\KeywordTok{rdf:RDF}\NormalTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

\normalsize

According to the RDF/XML specification, This imports several other
vocabularies behind a namespace. E.g., \texttt{rdf:type} refers to
\texttt{type} in the \texttt{rdf} namespace, which points to
\texttt{http://www.w3.org/1999/02/22-rdf-syntax-ns\#}. XML tags with no
namespace are resolved within the default namespace, which is our
proposed execution-description vocabulary.

\hypertarget{language-description}{%
\subsection{Language description}\label{language-description}}

At a very basic level, one could have commands and the purpose that they
serve:

\small

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{\textless{}}\KeywordTok{process}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{command}\NormalTok{\textgreater{}./execute {-}{-}input data.csv\textless{}/}\KeywordTok{command}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{purpose}\NormalTok{\textgreater{}generates data\textless{}/}\KeywordTok{purpose}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{process}\NormalTok{\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{process}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{command}\NormalTok{\textgreater{}python3 main.py\textless{}/}\KeywordTok{command}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{purpose}\NormalTok{\textgreater{}plots figures\textless{}/}\KeywordTok{purpose}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{process}\NormalTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

\normalsize

While we could define conventions around what to name the content of the
``purpose'' tag, it would be more powerful if the language could link
directly to the claims in the publication. The CiTO vocabulary
\cite{shotton_cito_2010} already defines a vocabulary for describing
citations.

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}}\KeywordTok{process}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{command}\NormalTok{\textgreater{}./execute {-}{-}input data.csv\textless{}/}\KeywordTok{command}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{purpose}\NormalTok{\textgreater{}}
    \CommentTok{\textless{}!{-}{-} links to an entire publication {-}{-}\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{cito:isCitedAsEvidenceBy}\OtherTok{ rdf:resource=}\StringTok{"https://doi.org/10.1234/123456789"}\NormalTok{ /\textgreater{}}
\NormalTok{  \textless{}/}\KeywordTok{purpose}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{process}\NormalTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

\normalsize

If the publisher hosts an RDF description at the URL
``https://doi.org/10.1234/123456789'' when the HTTP request content-type
header is \texttt{application/rdf+xml}, then this creates a web of
linked data. The publisher may have the title, authors, date published,
and other metadata using Dublin Core metadata terms, for example. This
is the dream of linked data: machine-readable data by different authors
hosted in different locations linking together seamlessly. Even if the
publisher does not have an RDF+XML description, third parties can make
claims about ``https://doi.org/10.1234/123456789'', although those
claims would not be as easily discoverable. One could even reference a
Nanopublication, which is a semantic web description of the scientific
claim.

The purpose description can be even more granular, using the DoCO
vocabulary \cite{constantin_document_2016}, which describes documents.

\small

\begin{verbatim}
<purpose>
  <prov:generated>
    <doco:figure>
      <dc:title>Figure 2b</dc:title>
      <dc:isPartOf rdf:resource="https://doi.org/10.1234/123456789" />
    </doco:figure>
  </prov:generated>
</purpose>
\end{verbatim}

\normalsize

With this complete, anyone should be able to execute the experiments
which generate figures or claims in the paper if they are labeled by the
computational scientist in this language.

One can view specifying the software environment as just prerequisite
steps in the computational DAG.

\small

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}}\KeywordTok{process}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{wfdesc:hasOutput}\NormalTok{\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{wfdesc:Output}\OtherTok{ rdf:nodeID=}\StringTok{"conda{-}env{-}out"}\NormalTok{ /\textgreater{}}
\NormalTok{  \textless{}/}\KeywordTok{wfdesc:hasOutput}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{command}\NormalTok{\textgreater{}conda env create {-}{-}name experiment{-}123 {-}{-}file environment.yml\textless{}/}\KeywordTok{command}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{process}\NormalTok{\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{process}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{wfdesc:hasInput}\NormalTok{\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{wfdesc:Input}\OtherTok{ rdf:nodeID=}\StringTok{"conda{-}env{-}in"}\NormalTok{ /\textgreater{}}
\NormalTok{  \textless{}/}\KeywordTok{wfdesc:hasOutput}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{purpose}\NormalTok{\textgreater{}figure 4\textless{}/}\KeywordTok{purpose}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{command}\NormalTok{\textgreater{}conda run {-}{-}name experiment{-}123 ./plot{-}figure.py\textless{}/}\KeywordTok{command}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{process}\NormalTok{\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{wfdesc:DataLink}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{wfdesc:hasSource}\OtherTok{ rdf:nodeID=}\StringTok{"xy{-}dataset{-}out"}\NormalTok{ /\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{wfdesc:hasSink}\OtherTok{ rdf:nodeID=}\StringTok{"xy{-}dataset{-}in"}\NormalTok{ /\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{wfdesc:DataLink}\NormalTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

\normalsize

Now a machine knows that the \texttt{conda\ env\ create\ ...} must be
run before, and the script should be run within the resulting conda
environment.

The purpose of an execution language is not to usurp the build-system or
workflow engine, which both already handle task DAGs; there must be some
minimal support for DAGs just for the cases where the DAG of tasks is
not already encoded in a build-system or workflow engine.

In addition to specifying the computational environment and command to
run, this language is an ideal candidate to also describe the parameters
of the experiment, like: One could specify range of valid values or list
options. With this complete, one can even do automated parameter-space
search studies, multi-fidelity uncertainty quantification, automated
outcome-preserving input minimization, and other automatic experiments.

Retrospective provenance seeks to encode how we got to a specific
result. Developers can put summary statistics of intermediate results
into the provenance description; if re-executions diverge, users can
locate which stage amplifies error the most. A tool might use
system-call interposition to learn about a processes reads, writes, and
forks. This would be better at identifying and recording intermediate
results. When reproducing some computational experiment, users can check
the intermediate results to see where they begin to differ. Wfprov is
one vocabulary for specifying retrospective provenance, and there is
already an experimental plugin for Nextflow which targets wfprov
\cite{grande_nf-prov_2023}.

Users may also want to know how much computational resources (CPU time,
disk space, and RAM) the computational experiment requires. Provenance
is the ideal place for computational experiments who already ran the
experiment to put this information. This way, users seeking to reproduce
the computational experiment know how many resources to request
(ahead-of-time allocations are usuually required for batch-scheduled
machines).

\hypertarget{making-easy-on-ramps-for-adoption}{%
\section{Making easy on-ramps for
adoption}\label{making-easy-on-ramps-for-adoption}}

The execution language should seek to describe existing software
frameworks, not replace them. In particular, execution should not
replace workflow engines. They should instead be wrapped as
process-nodes within the execution language. Computational experiments
can continue using their existing build-system and workflow.

A execution description could even be ``captured'' from an interactive
shell session with the user. They would invoke a shell that records
every command, its exit status, its read-files, and its write-files
(using syscall interposition). The user would execute their build-system
like normal within this shell. When the user exits, the shell will
create a DAG based on the read-files and write-files. For each output
file that is not consumed by another command, the shell would prompt the
user to describe that command's purpose. Finally, the shell would output
a execution description.

In cases where the description cannot be captured by an interactive
shell session, one can encode the system of logic that humans would use
to deduce the experimental structure. This is similar to the approach
taken by FlaPy \cite{gruber_empirical_2021}, a large-scale re-execution
study for Python unittests, to install the Python environment. For
example,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If \texttt{shell.nix}, \texttt{flake.nix}, or \texttt{environment.yml}
  exists, then use Nix, Nix flakes, or Conda as the environment for
  future commands.
\item
  If the environment is not already set, and \texttt{requirements.txt}
  exists, use Pip and Virtualenv as the environment for future commands.
\item
  If \texttt{CMakeLists.txt} exists, set \texttt{cmake} and
  \texttt{make} as commands.
\item
  If \texttt{configure.sh} exists, set \texttt{./configure} and
  \texttt{make} as command.
\item
  If \texttt{Makefile} exists, set \texttt{make} as command. \ldots{}
\end{enumerate}

In the best case, the execution description would be uploaded to the
same repository or location that contains the source for the
computational experiment. This way it can be maintained and used by the
original developers, and it is easily discoverable by users.
Alternatively, execution descriptions could be placed in a central
execution-description repository owned by software-engineering
researchers. The execution description is linked data, so it can live
anywhere, and existing strategies for finding, filtering, and trusting
linked data sets would work for execution descriptions.

This is similar to the approach taken by Python for type annotations.
Type annotations are easiest to maintain in the original repository, but
if the original repository rejects type annotations, there is still a
home for them in typeshed.

\hypertarget{is-this-another-competing-standard}{%
\section{Is this another competing
standard?}\label{is-this-another-competing-standard}}

It is something extra users have to do, but it does not attempt to
displace their existing practice.

\hypertarget{incentives-for-computational-scientists-to-maintain-execution-descriptions}{%
\section{Incentives for computational scientists to maintain execution
descriptions}\label{incentives-for-computational-scientists-to-maintain-execution-descriptions}}

Computational scientists are incentivized to describe their project this
way to benefit from the work of software-engineering researchers. When
software-engineering researchers do a large-scale execution study, it is
a ``free'' reproduction of their work.

To get an artifact evaluation badge, normally computational scientists
would have to write a natural language description of what the software
environment, what the commands are, how to run them, and where does the
data end up. The artifact evaluator has to read, interpret, and execute
their description by hand. An execution description could make this
nearly automatic; if a execution description exists, the artifact
evaluator uses an executor which understands the language and runs all
of the commands that reference the manuscript in their \texttt{purpose}
tag. The only manual labor is comparing these results to those in the
paper. Even that comparison can be simplified, if the last step in the
execution description outputs a boolean representing ``is the hypothesis
proven?''; the reviewer just needs to see that all of these output
``true''.



%%\printbibliography
%
\bibliographystyle{plain}
\bibliography{main.bib}

\end{document}
