
@article{collberg_repeatability_2016,
	title = {Repeatability in computer systems research},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2812803},
	doi = {10.1145/2812803},
	abstract = {To encourage repeatable research, fund repeatability engineering and reward commitments to sharing research artifacts.},
	language = {en},
	number = {3},
	urldate = {2022-05-27},
	journal = {Communications of the ACM},
	author = {Collberg, Christian and Proebsting, Todd A.},
	month = feb,
	year = {2016},
	keywords = {internship-project, project-acm-rep, project-provenance-pp, research software engineering},
	pages = {62--69},
	file = {2812803.pdf:/home/sam/Zotero/storage/JGDDR733/2812803.pdf:application/pdf},
}

@inproceedings{elsner_empirically_2021,
	address = {New York, NY, USA},
	series = {{ISSTA} 2021},
	title = {Empirically evaluating readily available information for regression test optimization in continuous integration},
	isbn = {978-1-4503-8459-9},
	url = {https://doi.org/10.1145/3460319.3464834},
	doi = {10.1145/3460319.3464834},
	abstract = {Regression test selection (RTS) and prioritization (RTP) techniques aim to reduce testing efforts and developer feedback time after a change to the code base. Using various information sources, including test traces, build dependencies, version control data, and test histories, they have been shown to be effective. However, not all of these sources are guaranteed to be available and accessible for arbitrary continuous integration (CI) environments. In contrast, metadata from version control systems (VCSs) and CI systems are readily available and inexpensive. Yet, corresponding RTP and RTS techniques are scattered across research and often only evaluated on synthetic faults or in a specific industrial context. It is cumbersome for practitioners to identify insights that apply to their context, let alone to calibrate associated parameters for maximum cost-effectiveness. This paper consolidates existing work on RTP and unsafe RTS into an actionable methodology to build and evaluate such approaches that exclusively rely on CI and VCS metadata. To investigate how these approaches from prior research compare in heterogeneous settings, we apply the methodology in a large-scale empirical study on a set of 23 projects covering 37,000 CI logs and 76,000 VCS commits. We find that these approaches significantly outperform established RTP baselines and, while still triggering 90\% of the failures, we show that practitioners can expect to save on average 84\% of test execution time for unsafe RTS. We also find that it can be beneficial to limit training data, features from test history work better than change-based features, and, somewhat surprisingly, simple and well-known heuristics often outperform complex machine-learned models.},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the 30th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Elsner, Daniel and Hauer, Florian and Pretschner, Alexander and Reimer, Silke},
	month = jul,
	year = {2021},
	note = {interest: 99},
	keywords = {continuous integration, project-acm-rep, regression testing, project-provenance-pp},
	pages = {491--504},
	file = {Full Text PDF:/home/sam/Zotero/storage/CG4ZZ7MN/Elsner et al. - 2021 - Empirically evaluating readily available informati.pdf:application/pdf},
}

@inproceedings{rougier_rescience_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ReScience} {C}: {A} {Journal} for {Reproducible} {Replications} in {Computational} {Science}},
	isbn = {978-3-030-23987-9},
	shorttitle = {{ReScience} {C}},
	doi = {10.1007/978-3-030-23987-9_14},
	abstract = {Independent replication is one of the most powerful methods to verify published scientific studies. In computational science, it requires the reimplementation of the methods described in the original article by a different team of researchers. Replication is often performed by scientists who wish to gain a better understanding of a published method, but its results are rarely made public. ReScience C is a peer-reviewed journal dedicated to the publication of high-quality computational replications that provide added value to the scientific community. To this end, ReScience C requires replications to be reproducible and implemented using Open Source languages and libraries. In this article, we provide an overview of ReScience C’s goals and quality standards, outline the submission and reviewing processes, and summarize the experience of its first three years of operation, concluding with an outlook towards evolutions envisaged for the near future.},
	language = {en},
	booktitle = {Reproducible {Research} in {Pattern} {Recognition}},
	publisher = {Springer International Publishing},
	author = {Rougier, Nicolas P. and Hinsen, Konrad},
	editor = {Kerautret, Bertrand and Colom, Miguel and Lopresti, Daniel and Monasse, Pascal and Talbot, Hugues},
	year = {2019},
	keywords = {reproducibility engineering, project-acm-rep, project-provenance-pp},
	pages = {150--156},
	file = {Submitted Version:/home/sam/Zotero/storage/3YZGFA62/Rougier and Hinsen - 2019 - ReScience C A Journal for Reproducible Replicatio.pdf:application/pdf},
}

@article{krafczyk_learning_2021,
	title = {Learning from reproducing computational results: introducing three principles and the {Reproduction} {Package}},
	volume = {379},
	shorttitle = {Learning from reproducing computational results},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0069},
	doi = {10.1098/rsta.2020.0069},
	abstract = {We carry out efforts to reproduce computational results for seven published articles and identify barriers to computational reproducibility. We then derive three principles to guide the practice and dissemination of reproducible computational research: (i) Provide transparency regarding how computational results are produced; (ii) When writing and releasing research software, aim for ease of (re-)executability; (iii) Make any code upon which the results rely as deterministic as possible. We then exemplify these three principles with 12 specific guidelines for their implementation in practice. We illustrate the three principles of reproducible research with a series of vignettes from our experimental reproducibility work. We define a novel Reproduction Package, a formalism that specifies a structured way to share computational research artifacts that implements the guidelines generated from our reproduction efforts to allow others to build, reproduce and extend computational science. We make our reproduction efforts in this paper publicly available as exemplar Reproduction Packages.

This article is part of the theme issue ‘Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico’.},
	number = {2197},
	urldate = {2023-01-31},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Krafczyk, M. S. and Shi, A. and Bhaskar, A. and Marinov, D. and Stodden, V.},
	month = mar,
	year = {2021},
	note = {Publisher: Royal Society},
	keywords = {reproducibility engineering, project-acm-rep, project-provenance-pp},
	pages = {20200069},
	file = {Full Text PDF:/home/sam/Zotero/storage/P3RYMMMA/Krafczyk et al. - 2021 - Learning from reproducing computational results i.pdf:application/pdf},
}

@inproceedings{gomez-perez_when_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {When {History} {Matters} - {Assessing} {Reliability} for the {Reuse} of {Scientific} {Workflows}},
	isbn = {978-3-642-41338-4},
	doi = {10.1007/978-3-642-41338-4_6},
	abstract = {Scientific workflows play an important role in computational research as essential artifacts for communicating the methods used to produce research findings. We are witnessing a growing number of efforts that treat workflows as first-class artifacts for sharing and exchanging scientific knowledge, either as part of scholarly articles or as stand-alone objects. However, workflows are not born to be reliable, which can seriously damage their reusability and trustworthiness as knowledge exchange instruments. Scientific workflows are commonly subject to decay, which consequently undermines their reliability over their lifetime. The reliability of workflows can be notably improved by advocating scientists to preserve a minimal set of information that is essential to assist the interpretations of these workflows and hence improve their potential for reproducibility and reusability. In this paper we show how, by measuring and monitoring the completeness and stability of scientific workflows over time we are able to provide scientists with a measure of their reliability, supporting the reuse of trustworthy scientific knowledge.},
	language = {en},
	booktitle = {The {Semantic} {Web} – {ISWC} 2013},
	publisher = {Springer},
	author = {Gómez-Pérez, José Manuel and García-Cuesta, Esteban and Garrido, Aleix and Ruiz, José Enrique and Zhao, Jun and Klyne, Graham},
	editor = {Alani, Harith and Kagal, Lalana and Fokoue, Achille and Groth, Paul and Biemann, Chris and Parreira, Josiane Xavier and Aroyo, Lora and Noy, Natasha and Welty, Chris and Janowicz, Krzysztof},
	year = {2013},
	keywords = {workflow managers, reproducibility, project-acm-rep, project-provenance-pp},
	pages = {81--97},
}

@inproceedings{butt_provone_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ProvONE}+: {A} {Provenance} {Model} for {Scientific} {Workflows}},
	isbn = {978-3-030-62008-0},
	shorttitle = {{ProvONE}+},
	doi = {10.1007/978-3-030-62008-0_30},
	abstract = {The provenance of workflows is essential, both for the data they derive and for their specification, to allow for the reproducibility, sharing and reuse of information in the scientific community. Although the formal modelling of scientific workflow provenance was of interest and studied, in many fields like semantic web, yet no provenance model has existed, we are aware of, to model control-flow driven scientific workflows. The provenance models proposed by the semantic web community for data-driven scientific workflows may capture the provenance of control-flow driven workflows execution traces (i.e., retrospective provenance) but underspecify the workflow structure (i.e., workflow provenance). An underspecified or incomplete structure of a workflow results in the misinterpretation of a scientific experiment and precludes conformance checking of the workflow, thereby restricting the gains of provenance. To overcome the limitation, we present a formal, lightweight and general-purpose specification model for the control-flows involved scientific workflows. The proposed model can be combined with the existing provenance models and easy to extend to specify the common control-flow patterns. In this article, we inspire the need for control-flow driven scientific workflow provenance model, provide an overview of its key classes and properties, and briefly discuss its integration with the ProvONE provenance model as well as its compatibility to PROV-DM. We will also focus on the sample modelling using the proposed model and present a comprehensive implementation scenario from the agricultural domain for validating the model.},
	language = {en},
	booktitle = {Web {Information} {Systems} {Engineering} – {WISE} 2020},
	publisher = {Springer International Publishing},
	author = {Butt, Anila Sahar and Fitch, Peter},
	editor = {Huang, Zhisheng and Beek, Wouter and Wang, Hua and Zhou, Rui and Zhang, Yanchun},
	year = {2020},
	keywords = {semantic web, provenance, project-acm-rep, project-provenance-pp},
	pages = {431--444},
	file = {Full Text PDF:/home/sam/Zotero/storage/XIV45RJ9/Butt and Fitch - 2020 - ProvONE+ A Provenance Model for Scientific Workfl.pdf:application/pdf},
}

@article{oliveira_provenance_2018,
	title = {Provenance {Analytics} for {Workflow}-{Based} {Computational} {Experiments}: {A} {Survey}},
	volume = {51},
	issn = {0360-0300},
	shorttitle = {Provenance {Analytics} for {Workflow}-{Based} {Computational} {Experiments}},
	url = {https://doi.org/10.1145/3184900},
	doi = {10.1145/3184900},
	abstract = {Until not long ago, manually capturing and storing provenance from scientific experiments were constant concerns for scientists. With the advent of computational experiments (modeled as scientific workflows) and Scientific Workflow Management Systems, produced and consumed data, as well as the provenance of a given experiment, are automatically managed, so provenance capturing and storing in such a context is no longer a major concern. Similarly to several existing big data problems, the bottom line is now on how to analyze the large amounts of provenance data generated by workflow executions and how to be able to extract useful knowledge of this data. In this context, this article surveys the current state of the art on provenance analytics by presenting the key initiatives that have been taken to support provenance data analysis. We also contribute by proposing a taxonomy to classify elements related to provenance analytics.},
	number = {3},
	urldate = {2023-02-23},
	journal = {ACM Computing Surveys},
	author = {Oliveira, Wellington and Oliveira, Daniel De and Braganholo, Vanessa},
	month = may,
	year = {2018},
	note = {interest: 95},
	keywords = {provenance, project-provenance-pp},
	pages = {53:1--53:25},
	file = {Full Text PDF:/home/sam/Zotero/storage/IT8P4NF7/Oliveira et al. - 2018 - Provenance Analytics for Workflow-Based Computatio.pdf:application/pdf},
}

@article{howison_retract_2014,
	title = {Retract bit-rotten publications: {Aligning} incentives for sustaining scientific software},
	shorttitle = {Retract bit-rotten publications},
	url = {https://figshare.com/articles/journal_contribution/Retract_bit_rotten_publications_Aligning_incentives_for_sustaining_scientific_software/1111632/1},
	doi = {10.6084/m9.figshare.1111632.v1},
	abstract = {A provocation for the WSSSPE2 workshop},
	language = {en},
	urldate = {2023-02-23},
	author = {Howison, James},
	month = jul,
	year = {2014},
	note = {Publisher: figshare},
	keywords = {continuous integration, reproducibility engineering, project-provenance-pp},
	file = {Snapshot:/home/sam/Zotero/storage/7VJNSRVB/1111632.html:text/html},
}

@misc{noauthor_confirmation_2014,
	title = {Confirmation {Depth} as a measure of reproducible scientific research.},
	url = {http://davidsoergel.com/posts/confirmation-depth-as-a-measure-of-reproducible-scientific-research},
	abstract = {What does it mean to reproduce a scientific study?  Confirmation Depth provides a guiding principle.},
	language = {en},
	urldate = {2023-02-23},
	journal = {David Soergel},
	month = oct,
	year = {2014},
	keywords = {project-provenance-pp},
	file = {Snapshot:/home/sam/Zotero/storage/KNRLR2QW/confirmation-depth-as-a-measure-of-reproducible-scientific-research.html:text/html},
}

@misc{mesnard_reproducible_2016,
	title = {Reproducible and replicable {CFD}: it's harder than you think},
	shorttitle = {Reproducible and replicable {CFD}},
	url = {http://arxiv.org/abs/1605.04339},
	doi = {10.48550/arXiv.1605.04339},
	abstract = {Completing a full replication study of our previously published findings on bluff-body aerodynamics was harder than we thought. Despite the fact that we have good reproducible-research practices, sharing our code and data openly. Here's what we learned from three years, four CFD codes and hundreds of runs.},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Mesnard, Olivier and Barba, Lorena A.},
	month = oct,
	year = {2016},
	note = {arXiv:1605.04339 [physics]
interest: 94},
	keywords = {reproducibility engineering, project-provenance-pp},
	file = {arXiv Fulltext PDF:/home/sam/Zotero/storage/4C9ZRCHY/Mesnard and Barba - 2016 - Reproducible and replicable CFD it's harder than .pdf:application/pdf;arXiv.org Snapshot:/home/sam/Zotero/storage/7KT47NJL/1605.html:text/html},
}

@article{timperley_understanding_2021,
	title = {Understanding and {Improving} {Artifact} {Sharing} in {Software} {Engineering} {Research}},
	volume = {26},
	issn = {1382-3256, 1573-7616},
	url = {http://arxiv.org/abs/2008.01046},
	doi = {10.1007/s10664-021-09973-5},
	abstract = {In recent years, many software engineering researchers have begun to include artifacts alongside their research papers. Ideally, artifacts, including tools, benchmarks, and data, support the dissemination of ideas, provide evidence for research claims, and serve as a starting point for future research. However, in practice, artifacts suffer from a variety of issues that prevent the realization of their full potential. To help the software engineering community realize the full potential of artifacts, we seek to understand the challenges involved in the creation, sharing, and use of artifacts. To that end, we perform a mixed-methods study including a survey of artifacts in software engineering publications, and an online survey of 153 software engineering researchers. By analyzing the perspectives of artifact creators, users, and reviewers, we identify several high-level challenges that affect the quality of artifacts including mismatched expectations between these groups, and a lack of sufficient reward for both creators and reviewers. Using Diffusion of Innovations as an analytical framework, we examine how these challenges relate to one another, and build an understanding of the factors that affect the sharing and success of artifacts. Finally, we make recommendations to improve the quality of artifacts based on our results and existing best practices.},
	number = {4},
	urldate = {2023-05-06},
	journal = {Empirical Software Engineering},
	author = {Timperley, Christopher S. and Herckis, Lauren and Goues, Claire Le and Hilton, Michael},
	month = jul,
	year = {2021},
	note = {interest: 90
arXiv:2008.01046 [cs]},
	keywords = {research software engineering, reproducibility engineering, project-provenance-pp, artifact evaluation},
	pages = {67},
	file = {arXiv Fulltext PDF:/home/sam/Zotero/storage/BM7CHTK8/Timperley et al. - 2021 - Understanding and Improving Artifact Sharing in So.pdf:application/pdf;arXiv.org Snapshot:/home/sam/Zotero/storage/ID3NVV6E/2008.html:text/html},
}

@article{constantin_document_2016,
	title = {The {Document} {Components} {Ontology} ({DoCO})},
	volume = {7},
	issn = {1570-0844},
	url = {https://content.iospress.com/articles/semantic-web/sw177},
	doi = {10.3233/SW-150177},
	abstract = {The availability in machine-readable form of descriptions of the structure of documents, as well as of the document discourse (e.g. the scientific discourse within scholarly articles), is crucial for facilitating semantic publishing and the overall c},
	language = {en},
	number = {2},
	urldate = {2023-05-25},
	journal = {Semantic Web},
	author = {Constantin, Alexandru and Peroni, Silvio and Pettifer, Steve and Shotton, David and Vitali, Fabio},
	month = jan,
	year = {2016},
	note = {Publisher: IOS Press},
	keywords = {project-provenance-pp, semantic web},
	pages = {167--181},
	file = {Full Text PDF:/home/sam/Zotero/storage/AK4CAYTZ/Constantin et al. - 2016 - The&nbsp\;Document&nbsp\;Components&nbsp\;Ontology&nb.pdf:application/pdf},
}

@article{shotton_cito_2010,
	title = {{CiTO}, the {Citation} {Typing} {Ontology}},
	volume = {1},
	issn = {2041-1480},
	url = {https://doi.org/10.1186/2041-1480-1-S1-S6},
	doi = {10.1186/2041-1480-1-S1-S6},
	abstract = {CiTO, the Citation Typing Ontology, is an ontology for describing the nature of reference citations in scientific research articles and other scholarly works, both to other such publications and also to Web information resources, and for publishing these descriptions on the Semantic Web. Citation are described in terms of the factual and rhetorical relationships between citing publication and cited publication, the in-text and global citation frequencies of each cited work, and the nature of the cited work itself, including its publication and peer review status. This paper describes CiTO and illustrates its usefulness both for the annotation of bibliographic reference lists and for the visualization of citation networks. The latest version of CiTO, which this paper describes, is CiTO Version 1.6, published on 19 March 2010. CiTO is written in the Web Ontology Language OWL, uses the namespace http://purl.org/net/cito/, and is available from http://purl.org/net/cito/. This site uses content negotiation to deliver to the user an OWLDoc Web version of the ontology if accessed via a Web browser, or the OWL ontology itself if accessed from an ontology management tool such as Protégé 4 (http://protege.stanford.edu/). Collaborative work is currently under way to harmonize CiTO with other ontologies describing bibliographies and the rhetorical structure of scientific discourse.},
	language = {en},
	number = {1},
	urldate = {2023-05-25},
	journal = {Journal of Biomedical Semantics},
	author = {Shotton, David},
	month = jun,
	year = {2010},
	keywords = {semantic web, project-provenance-pp},
	pages = {S6},
	file = {Full Text PDF:/home/sam/Zotero/storage/NBZPC9S5/Shotton - 2010 - CiTO, the Citation Typing Ontology.pdf:application/pdf},
}

@misc{grande_nf-prov_2023,
	title = {nf-prov},
	copyright = {Apache-2.0},
	url = {https://github.com/Sage-Bionetworks-Workflows/nf-prov},
	urldate = {2023-05-25},
	publisher = {Sage-Bionetworks-Workflows},
	author = {Grande, Bruno and Sherman, Ben and Di Tomasso, Paolo},
	month = may,
	year = {2023},
	note = {original-date: 2022-12-19T21:16:30Z},
	keywords = {project-provenance-pp, provenance},
}

@inproceedings{gruber_empirical_2021,
	title = {An {Empirical} {Study} of {Flaky} {Tests} in {Python}},
	doi = {10.1109/ICST49551.2021.00026},
	abstract = {Tests that cause spurious failures without any code changes, i.e., flaky tests, hamper regression testing, increase maintenance costs, may shadow real bugs, and decrease trust in tests. While the prevalence and importance of flakiness is well established, prior research focused on Java projects, thus raising the question of how the findings generalize. In order to provide a better understanding of the role of flakiness in software development beyond Java, we empirically study the prevalence, causes, and degree of flakiness within software written in Python, one of the currently most popular programming languages. For this, we sampled 22 352 open source projects from the popular PyPI package index, and analyzed their 876 186 test cases for flakiness. Our investigation suggests that flakiness is equally prevalent in Python as it is in Java. The reasons, however, are different: Order dependency is a much more dominant problem in Python, causing 59 \% of the 7 571 flaky tests in our dataset. Another 28 \% were caused by test infrastructure problems, which represent a previously undocumented cause of flakiness. The remaining 13 \% can mostly be attributed to the use of network and randomness APIs by the projects, which is indicative of the type of software commonly written in Python. Our data also suggests that finding flaky tests requires more runs than are often done in the literature: A 95 \% confidence that a passing test case is not flaky on average would require 170 reruns.},
	booktitle = {2021 14th {IEEE} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Gruber, Martin and Lukasczyk, Stephan and Kroiß, Florian and Fraser, Gordon},
	month = apr,
	year = {2021},
	note = {ISSN: 2159-4848},
	keywords = {project-provenance-pp, software mining, software testing},
	pages = {148--158},
	file = {IEEE Xplore Abstract Record:/home/sam/Zotero/storage/ZM52F9IF/9438576.html:text/html;IEEE Xplore Full Text PDF:/home/sam/Zotero/storage/A2WGSNPH/Gruber et al. - 2021 - An Empirical Study of Flaky Tests in Python.pdf:application/pdf},
}

@article{weibel_dublin_2000,
	title = {The {Dublin} {Core} {Metadata} {Initiative}: {Mission}, {Current} {Activities}, and {Future} {Directions}},
	volume = {6},
	issn = {1082-9873},
	shorttitle = {The {Dublin} {Core} {Metadata} {Initiative}},
	url = {http://www.dlib.org/dlib/december00/weibel/12weibel.html},
	doi = {10.1045/december2000-weibel},
	language = {en},
	number = {12},
	urldate = {2023-05-25},
	journal = {D-Lib Magazine},
	author = {Weibel, Stuart L. and Koch, Traugott},
	month = dec,
	year = {2000},
	keywords = {project-provenance-pp, semantic web},
}

@misc{gandon_rdf_2014,
	title = {{RDF} 1.1 {XML} {Syntax}},
	url = {https://www.w3.org/TR/rdf-syntax-grammar/#section-Syntax-blank-nodes},
	abstract = {This document defines an XML syntax for RDF called RDF/XML in terms of Namespaces in XML, the XML Information Set and XML Base.},
	urldate = {2023-05-26},
	journal = {W3C Standards},
	author = {Gandon, Fabian and Shcreiber, Guus and Beckett, David},
	month = feb,
	year = {2014},
	keywords = {project-provenance-pp, semantic web},
	file = {RDF 1.1 XML Syntax:/home/sam/Zotero/storage/XP5APX3P/rdf-syntax-grammar.html:text/html},
}

@article{groth_anatomy_2010,
	title = {The anatomy of a nanopublication},
	volume = {30},
	issn = {0167-5265},
	url = {https://content.iospress.com/articles/information-services-and-use/isu613},
	doi = {10.3233/ISU-2010-0613},
	abstract = {As the amount of scholarly communication increases, it is increasingly difficult for specific core scientific statements to be found, connected and curated. Additionally, the redundancy of these statements in multiple fora makes it difficult to deter},
	language = {en},
	number = {1-2},
	urldate = {2023-05-26},
	journal = {Information Services \& Use},
	author = {Groth, Paul and Gibson, Andrew and Velterop, Jan},
	month = jan,
	year = {2010},
	note = {Publisher: IOS Press},
	keywords = {academic publishing, project-provenance-pp, semantic web},
	pages = {51--56},
	file = {Full Text PDF:/home/sam/Zotero/storage/LLFTU3XH/Groth et al. - 2010 - The anatomy of a nanopublication.pdf:application/pdf},
}

@inproceedings{erxleben_introducing_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Introducing {Wikidata} to the {Linked} {Data} {Web}},
	isbn = {978-3-319-11964-9},
	doi = {10.1007/978-3-319-11964-9_4},
	abstract = {Wikidata is the central data management platform of Wikipedia. By the efforts of thousands of volunteers, the project has produced a large, open knowledge base with many interesting applications. The data is highly interlinked and connected to many other datasets, but it is also very rich, complex, and not available in RDF. To address this issue, we introduce new RDF exports that connect Wikidata to the Linked Data Web. We explain the data model of Wikidata and discuss its encoding in RDF. Moreover, we introduce several partial exports that provide more selective or simplified views on the data. This includes a class hierarchy and several other types of ontological axioms that we extract from the site. All datasets we discuss here are freely available online and updated regularly.},
	language = {en},
	booktitle = {The {Semantic} {Web} – {ISWC} 2014},
	publisher = {Springer International Publishing},
	author = {Erxleben, Fredo and Günther, Michael and Krötzsch, Markus and Mendez, Julian and Vrandečić, Denny},
	editor = {Mika, Peter and Tudorache, Tania and Bernstein, Abraham and Welty, Chris and Knoblock, Craig and Vrandečić, Denny and Groth, Paul and Noy, Natasha and Janowicz, Krzysztof and Goble, Carole},
	year = {2014},
	keywords = {project-provenance-pp, semantic web},
	pages = {50--65},
	file = {Full Text PDF:/home/sam/Zotero/storage/QZACD8IN/Erxleben et al. - 2014 - Introducing Wikidata to the Linked Data Web.pdf:application/pdf},
}

@article{soiland-reyes_wf4ever_2013,
	title = {{Wf4Ever} {Research} {Object} {Model}},
	copyright = {Creative Commons Attribution 4.0, Open Access},
	url = {https://zenodo.org/record/12744},
	doi = {10.5281/ZENODO.12744},
	abstract = {The Wf4Ever Research Object Model provides a vocabulary for the description of workflow-centric Research Objects: aggregations of resources relating to scientific workflows.

{\textless}strong{\textgreater}Permalink{\textless}/strong{\textgreater}: https://w3id.org/ro/2013-11-30/},
	urldate = {2023-05-26},
	author = {Soiland-Reyes, Stian and Bechhofer, Sean and Belhajjame, Khalid and Klyne, Graham and Garijo, Daniel and Coricho, Oscar and García Cuesta, Esteban and Palma, Raul},
	month = nov,
	year = {2013},
	note = {Publisher: Zenodo},
	keywords = {project-provenance-pp, provenance, semantic web},
}

@inproceedings{garijo_new_2011,
	address = {New York, NY, USA},
	series = {{WORKS} '11},
	title = {A new approach for publishing workflows: abstractions, standards, and linked data},
	isbn = {978-1-4503-1100-7},
	shorttitle = {A new approach for publishing workflows},
	url = {https://dl.acm.org/doi/10.1145/2110497.2110504},
	doi = {10.1145/2110497.2110504},
	abstract = {In recent years, a variety of systems have been developed that export the workflows used to analyze data and make them part of published articles. We argue that the workflows that are published in current approaches are dependent on the specific codes used for execution, the specific workflow system used, and the specific workflow catalogs where they are published. In this paper, we describe a new approach that addresses these shortcomings and makes workflows more reusable through: 1) the use of abstract workflows to complement executable workflows to make them reusable when the execution environment is different, 2) the publication of both abstract and executable workflows using standards such as the Open Provenance Model that can be imported by other workflow systems, 3) the publication of workflows as Linked Data that results in open web accessible workflow repositories. We illustrate this approach using a complex workflow that we re-created from an influential publication that describes the generation of 'drugomes'.},
	urldate = {2023-05-26},
	booktitle = {Proceedings of the 6th workshop on {Workflows} in support of large-scale science},
	publisher = {Association for Computing Machinery},
	author = {Garijo, Daniel and Gil, Yolanda},
	month = nov,
	year = {2011},
	keywords = {project-provenance-pp, semantic web, workflow managers},
	pages = {47--56},
	file = {Full Text PDF:/home/sam/Zotero/storage/5FU8H8X6/Garijo and Gil - 2011 - A new approach for publishing workflows abstracti.pdf:application/pdf},
}

@article{soiland-reyes_packaging_2022,
	title = {Packaging research artefacts with {RO}-{Crate}},
	volume = {5},
	issn = {2451-8484},
	url = {https://content.iospress.com/articles/data-science/ds210053},
	doi = {10.3233/DS-210053},
	abstract = {An increasing number of researchers support reproducibility by including pointers to and descriptions of datasets, software and methods in their publications. However, scientific articles may be ambiguous, incomplete and difficult to process by autom},
	language = {en},
	number = {2},
	urldate = {2023-05-26},
	journal = {Data Science},
	author = {Soiland-Reyes, Stian and Sefton, Peter and Crosas, Mercè and Castro, Leyla Jael and Coppens, Frederik and Fernández, José M. and Garijo, Daniel and Grüning, Björn and La Rosa, Marco and Leo, Simone and Ó Carragáin, Eoghan and Portier, Marc and Trisovic, Ana and RO-Crate Community and Groth, Paul and Goble, Carole},
	month = jan,
	year = {2022},
	note = {Publisher: IOS Press},
	keywords = {project-provenance-pp, reproducibility engineering},
	pages = {97--138},
	file = {Full Text PDF:/home/sam/Zotero/storage/YHYB8T2Y/Soiland-Reyes et al. - 2022 - Packaging research artefacts with RO-Crate.pdf:application/pdf},
}

@inproceedings{gray_bioschemas_2017,
	title = {Bioschemas: {From} {Potato} {Salad} to {Protein} {Annotation}},
	shorttitle = {Bioschemas},
	url = {https://research.manchester.ac.uk/en/publications/bioschemas-from-potato-salad-to-protein-annotation},
	language = {English},
	urldate = {2023-05-26},
	booktitle = {{ISWC} 2017 {Posters} \& {Demonstrations} and {Industry} {Tracks}: {Proceedings} of the {ISWC} 2017 {Posters} \& {Demonstrations} and {Industry} {Tracks} co-located with 16th {International} {Semantic} {Web} {Conference} ({ISWC} 2017)},
	publisher = {RWTH Aachen University},
	author = {Gray, Alasdair J. G. and Goble, Carole and Jimenez, Rafael C.},
	month = oct,
	year = {2017},
	keywords = {project-provenance-pp, semantic web},
	file = {Full Text PDF:/home/sam/Zotero/storage/XU99KNK2/Gray et al. - 2017 - Bioschemas From Potato Salad to Protein Annotatio.pdf:application/pdf},
}

@misc{wilder-james_description_2017,
	title = {Description of a {Project} wiki},
	url = {https://github.com/ewilderj/doap/wiki},
	abstract = {DOAP is a project to create an XML/RDF vocabulary to describe software projects, and in particular open source projects.

In addition to developing an RDF schema and examples, the DOAP project aims to provide tool support in all the popular programming languages.},
	urldate = {2023-05-26},
	author = {Wilder-James., Edd},
	month = jan,
	year = {2017},
	keywords = {project-provenance-pp, semantic web},
	file = {Home · ewilderj/doap Wiki:/home/sam/Zotero/storage/6H2FHPPR/wiki.html:text/html},
}
